{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb822e2f-029c-4f90-98d2-40a46334b6a7",
   "metadata": {},
   "source": [
    "# Youtube Ad Skipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb6d2cf-8ddf-4e27-80d8-7b1600dde998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optical Character Recognition (OCR) Library Install\n",
    "#1. Install GTK for Windows- https://github.com/tschoonj/GTK-for-Windows-Runtime-Environment-Installer/releases\n",
    "#2. Create a new venv\n",
    "# DocTR for PyTorch\n",
    "#3.pip install \"python-doctr[torch]\" pyautogui\n",
    "#Ref: https://github.com/mindee/doctr/issues/701\n",
    "\n",
    "import os\n",
    "GTK_FOLDER = r'C:\\Program Files\\GTK3-Runtime Win64\\bin'\n",
    "os.environ['PATH'] = GTK_FOLDER + os.pathsep + os.environ.get('PATH', '')\n",
    "os.environ['USE_TORCH'] = '1'\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9d945a-6f96-4dd0-a433-26b10ecefdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import time\n",
    "import datetime\n",
    "import pyautogui\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b95fed67-c1b0-4986-ad9e-258dcbe6cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When fail-safe mode is True, moving the mouse to the upper-left will abort your program:\n",
    "pyautogui.FAILSAFE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76438aa2-da27-45e6-8e8b-2681a64455df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Directory for Inputs: ./logs/logs_flow_input/yt_ad_skip\n",
      "Creating Directory for Outputs: ./logs/logs_flow_output/yt_ad_skip\n"
     ]
    }
   ],
   "source": [
    "#Load OCR Model from docTR: https://mindee.github.io/doctr/using_doctr/using_models.html\n",
    "#docTR_ocr_model = ocr_predictor(det_arch='db_resnet50', reco_arch='crnn_vgg16_bn', pretrained=True)\n",
    "\n",
    "#Half-precision (or FP16) is a binary floating-point format that occupies 16 bits in computer memory.\n",
    "##Advantages: Faster inference, Less memory usage\n",
    "### Ref: https://mindee.github.io/doctr/v0.7.0/using_doctr/using_model_export.html\n",
    "docTR_ocr_model = ocr_predictor(reco_arch=\"crnn_mobilenet_v3_small\", det_arch=\"linknet_resnet34\", pretrained=True).cuda().half()\n",
    "\n",
    "#Initialize Input and Output Paths\n",
    "flow_name= \"yt_ad_skip\" # get from config yaml\n",
    "\n",
    "# Create Input Logs Directory if not exists\n",
    "screenshot_store_path_prefix= \"./logs/logs_flow_input/\"\n",
    "flow_screenshot_store_path= os.path.join(screenshot_store_path_prefix, flow_name)\n",
    "print(\"Creating Directory for Inputs:\",flow_screenshot_store_path)\n",
    "os.makedirs(flow_screenshot_store_path, exist_ok=True)\n",
    "\n",
    "# Create Output Logs Directory if not exists\n",
    "scr_segments_store_path_prefix= \"./logs/logs_flow_output/\"\n",
    "flow_scr_segments_store_path= os.path.join(scr_segments_store_path_prefix, flow_name)\n",
    "print(\"Creating Directory for Outputs:\",flow_scr_segments_store_path)\n",
    "os.makedirs(flow_scr_segments_store_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a85366-82b1-4a2d-8ba5-38407e594f9c",
   "metadata": {},
   "source": [
    "## Get Youtube Screenshots every second. Check if \"Skip\" button is present and click on it\n",
    "\n",
    "Note: The latency/time taken for OCR completion is ~4 Seconds with CPU and ~1 Seconds with GPU with FP16 precision. So this demo may not satisy the constraint of taking action every second based on the hardware used to execute the program.\n",
    "- Run the OCR pipeline using CUDA GPUs to achieve the goal .\n",
    "- Current pipeline can be used as-is for RPA tasks for which few seconds of latency is acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f9493b9-e9c2-47f7-91ba-555e4af2b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR on the Screenshot is Complete. Time taken for OCR with CUDA FP16 is 863 ms.\n",
      "Creating Directory for Current Step Outputs: ./logs/logs_flow_output/yt_ad_skip\\step_0\n",
      "Found the keyword-  skip at:  [1146, 459, 1189, 482]\n"
     ]
    }
   ],
   "source": [
    "# For demo, wait until skip button comes on Youtube player and run this cell to trigger the automation, run for 1-step\n",
    "timeInSec_run_flow_step=1 \n",
    "\n",
    "# For demo, time to wait after User triggers this cell and switch the Active Window to Youtube player, verify if click is successful\n",
    "timeInSec_manually_switch_youtube_window=10\n",
    "time.sleep(timeInSec_manually_switch_youtube_window)\n",
    "\n",
    "for step_num in range(timeInSec_run_flow_step):\n",
    "    time.sleep(1)\n",
    "    curr_scr_shot_filename = os.path.join(flow_screenshot_store_path, str(step_num) + '.png')\n",
    "    get_screenshot= pyautogui.screenshot(curr_scr_shot_filename) \n",
    "    \n",
    "    # Perform OCR on the Screenshot and Search for the Keyword\n",
    "    search_keyword= \"skip\"\n",
    "    ocr_start_time = datetime.datetime.now()\n",
    "    \n",
    "    # Load the Screenshot\n",
    "    curr_screenshot_doc = DocumentFile.from_images(curr_scr_shot_filename)\n",
    "    ocr_result = docTR_ocr_model(curr_screenshot_doc)\n",
    "    \n",
    "    ocr_complete_time = datetime.datetime.now()\n",
    "    ocr_time_duration = ocr_complete_time - ocr_start_time\n",
    "    ocr_time_duration_milli_sec = round(ocr_time_duration.total_seconds()*1000)\n",
    "    print(f\"OCR on the Screenshot is Complete. Time taken for OCR with CUDA FP16 is {ocr_time_duration_milli_sec} ms.\")\n",
    "    \n",
    "    # Export the OCR Results and flatten it to find the keyword\n",
    "    json_output = ocr_result.export()\n",
    "    page_words = [[word for block in page['blocks'] for line in block['lines'] for word in line['words']] for page in json_output['pages']]\n",
    "    page_dims = [page['dimensions'] for page in json_output['pages']]\n",
    "    \n",
    "    # Get all the word coordinates in absolute dimensions, format in [xmin, ymin, xmax, ymax]\n",
    "    words_abs_coords = [\n",
    "        [[int(round(word['geometry'][0][0] * dims[1])), int(round(word['geometry'][0][1] * dims[0])), int(round(word['geometry'][1][0] * dims[1])), int(round(word['geometry'][1][1] * dims[0]))] for word in words]\n",
    "        for words, dims in zip(page_words, page_dims)\n",
    "    ]\n",
    "    \n",
    "    all_words_ocr=page_words[0]\n",
    "    all_words_coords= words_abs_coords[0]\n",
    "    num_words=len(all_words_ocr)\n",
    "    \n",
    "    # Create directory for current_step outputs if not exists\n",
    "    curr_step_directory = \"step_\"+str(step_num) \n",
    "    flow_curr_step_scr_segments_path= os.path.join(flow_scr_segments_store_path, curr_step_directory)\n",
    "    print(\"Creating Directory for Current Step Outputs:\",flow_curr_step_scr_segments_path)\n",
    "    os.makedirs(flow_curr_step_scr_segments_path, exist_ok=True)\n",
    "    \n",
    "    keyword_match_locs= []\n",
    "    curr_screenshot_image = cv2.imread(curr_scr_shot_filename)\n",
    "    for word_ind in range(num_words):\n",
    "        curr_word_location= all_words_coords[word_ind]\n",
    "        x1, y1, x2, y2 = curr_word_location\n",
    "        cropped_image = curr_screenshot_image[y1:y2, x1:x2]\n",
    "        cropped_image_filename = os.path.join(flow_curr_step_scr_segments_path, str(word_ind) + '.png')\n",
    "        cv2.imwrite(cropped_image_filename, cropped_image)\n",
    "        \n",
    "        curr_word_value= all_words_ocr[word_ind]['value']\n",
    "        curr_word_value_lower= curr_word_value.lower()\n",
    "        if search_keyword in curr_word_value_lower:\n",
    "            keyword_match_locs.append(curr_word_location)\n",
    "            \n",
    "    #To-do: Multiple matches of Keyword is not handled currently    \n",
    "    if len(keyword_match_locs) ==1 :\n",
    "        keyword_location= keyword_match_locs[0]\n",
    "        print(\"Found the keyword- \", search_keyword, \"at: \", keyword_location)\n",
    "        xmin, ymin, xmax, ymax= keyword_location\n",
    "        cx = (xmin + xmax) // 2\n",
    "        cy = (ymin + ymax) // 2\n",
    "        \n",
    "        # Move the mouse to cx, cy coordinates and click it.\n",
    "        pyautogui.click(cx, cy) \n",
    "    else:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9606a0-a699-430d-8c78-92ab594fe8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see the OCR Results with Bounding Boxes for all Detections\n",
    "#ocr_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
