{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc435429-a606-495e-a290-224fef079bb0",
   "metadata": {},
   "source": [
    "## Sample Python Notebook to perform Desktop RPA using Natural Language and Visual Instructions.\n",
    "This automation is powered by Opensource tools like OpenCV, Huggingface Transformers, Meta's SAM and DinoV2 models, Mindee's docTR models for OCR, PyAutoGUI for bot-actions and many incredible Python libraries.\n",
    "\n",
    "This framework is configuration driven, designed to be modular and open-ended.\n",
    "\n",
    "To get started, define the RPA bot action-steps aka flow using Yaml-based config file with Text and Image based instructions/prompts.\n",
    "\n",
    "Text Prompts can be a text field which can be interacted with on the UI\n",
    "\n",
    "Visual Prompts are images/icons which can interacted with on the UI. \n",
    "- They can be captured using Screen grab tools and saved as .jpg or .png format\n",
    "- For issues with detecting small objects, explore Accessibility Features like Magnifier, high contrast colors, available on the Desktop OS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e5849-5f07-4b78-8186-7923b88333ca",
   "metadata": {},
   "source": [
    "### Install the required Python Libraries (Expect installation hiccups on Windows OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a3fb7c-cc1f-4c2d-9a17-cef8de081bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optical Character Recognition (OCR) Library- docTR Installation Prerequisite\n",
    "# Ref: https://github.com/mindee/doctr/issues/701\n",
    "#1. Install GTK for Windows, downdload \"gtk3-runtime-3.24.31-2022-01-04-ts-win64.exe\"\n",
    "## Executable Available At- https://github.com/tschoonj/GTK-for-Windows-Runtime-Environment-Installer/releases\n",
    "#2. Create a new venv\n",
    "\n",
    "# PIP Install\n",
    "#pip install torch \"python-doctr[torch]\" pyautogui transformers opencv-python\n",
    "#pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041599a-2970-40ed-b077-7f8e45f4acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "# Set path for downstream library imports like docTR, Transformers\n",
    "GTK_FOLDER = r'C:\\Program Files\\GTK3-Runtime Win64\\bin'\n",
    "os.environ['PATH'] = GTK_FOLDER + os.pathsep + os.environ.get('PATH', '')\n",
    "os.environ['USE_TORCH'] = '1'\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342ce7b-5e0f-402a-8337-447550c04ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Remaining libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import pyautogui\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ed698-1047-4da5-9bda-04819112901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PyAutoGUI's fail-safe mode to True- moving the mouse to the upper-left will abort the program in case of infinite loop\n",
    "pyautogui.FAILSAFE = True\n",
    "\n",
    "#Set Device for Models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# For testing with CPU- remove it for code check-in\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203e2a9-ee59-4545-9210-67ddecd4d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a Pretrained SAM Model, from https://github.com/facebookresearch/segment-anything#model-checkpoints\n",
    "# Pass the model weights path to the model-loading step. This example uses \"vit_h\" model\n",
    "sam_models_root_path = \"C:/Users/mural/sam-models\"\n",
    "model_type = \"vit_h\"\n",
    "sam_model_checkpoint = sam_models_root_path+\"/sam_vit_h_4b8939.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcada95-50a4-4490-b89d-8b9f6c698f17",
   "metadata": {},
   "source": [
    "### Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728ab54-0c7e-46e6-95c1-f8ba9d75a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Prompts Support- Load OCR Model from docTR: https://mindee.github.io/doctr/using_doctr/using_models.html\n",
    "docTR_ocr_model = ocr_predictor(det_arch='db_resnet50', reco_arch='crnn_vgg16_bn', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb1483-b3ba-4f06-8d0b-93dd578a363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Prompts Support- Load the SAM model from Facebook/Meta\n",
    "# SAM Model- Used to Extract all the Objects (Image Patches/Masks) of Interest from the Image\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_model_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "# There are several tunable parameters in automatic mask generation\n",
    "# Default Segmentation Mask (Image Patch) Generator\n",
    "sam_mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "# Custom Segmentation Mask (Image Patch) Generator\n",
    "# Ref: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\n",
    "#sam_mask_generator = SamAutomaticMaskGenerator(\n",
    "#    model=sam,\n",
    "#    points_per_side=32,\n",
    "#    pred_iou_thresh=0.86,\n",
    "#    stability_score_thresh=0.92,\n",
    "#    crop_n_layers=1,\n",
    "#    crop_n_points_downscale_factor=2,\n",
    "#    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3a16a-17e5-4848-ae37-4eacdef965bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Prompts Support- Load the DinoV2 model from Facebook/Meta\n",
    "# DinoV2 Model- Used for Image Similarity via Feature based Template Matching\n",
    "## Ref: https://github.com/facebookresearch/dinov2\n",
    "\n",
    "dinov2_model = AutoModel.from_pretrained('facebook/dinov2-base').to(device)\n",
    "dinov2_processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f24cc-dfce-4526-8f39-52352f5169c5",
   "metadata": {},
   "source": [
    "### Set the directory paths for configs, image prompts, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ea770-b3d8-4caf-b313-ad0396dbd252",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_name= \"rpa_instruct_demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9cca9-a166-4ff8-ae39-484e0aa946c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Yaml Configuration File Path: Contains all the Steps and Actions for the Flow\n",
    "configs_path_prefix= \"./configs/\"\n",
    "config_file_name= \"/flow_config.yml\"\n",
    "flow_config_path= os.path.join(configs_path_prefix, flow_name)\n",
    "config_file_path= flow_config_path+config_file_name\n",
    "\n",
    "if not os.path.isfile(config_file_path):\n",
    "    print(\"Config File Not Found. Check the File Path and Rerun this Step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d18d00-e0b0-4a91-80ae-9546621bdfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Directory Containing the Images for the Icons/ Interactable Elements on the UI\n",
    "image_prompts_path_prefix= \"./image_prompts/\"\n",
    "image_prompts_dir= os.path.join(image_prompts_path_prefix, flow_name)\n",
    "\n",
    "if not os.path.isdir(image_prompts_dir):\n",
    "    print(\"Image Prompts Directory Not Found. Check the Folder Path and Rerun this Step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbab2f-0746-4c95-a48c-6f87e3ed8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Input and Output Paths needed for Logging\n",
    "# This provides the tracing for each step and helps in the explainability of each step's successful or failed action \n",
    "\n",
    "# Create Input Logs Directory if not exists :store each step's input screenshot\n",
    "screenshot_store_path_prefix= \"./logs/logs_flow_input/\"\n",
    "flow_screenshot_store_path= os.path.join(screenshot_store_path_prefix, flow_name)\n",
    "print(\"Creating Directory for Inputs:\",flow_screenshot_store_path)\n",
    "os.makedirs(flow_screenshot_store_path, exist_ok=True)\n",
    "\n",
    "# Create Output Logs Directory if not exists: store each step's processing results like image patches of text and/or icons\n",
    "scr_segments_store_path_prefix= \"./logs/logs_flow_output/\"\n",
    "flow_scr_segments_store_path= os.path.join(scr_segments_store_path_prefix, flow_name)\n",
    "print(\"Creating Directory for Outputs:\",flow_scr_segments_store_path)\n",
    "os.makedirs(flow_scr_segments_store_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c18b3-ba31-4aa9-b653-0d534d316422",
   "metadata": {},
   "source": [
    "### Validate Yaml, check all steps are in increasing order and all required fields are present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97731907-0e1e-4a11-8940-dcc3ef0dc393",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file_path, 'r') as file:\n",
    "    config_file = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b6d69-2591-408f-9daf-2fcf5cbfa1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpa_flow_steps= config_file['Flow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884bf79-93d7-4551-9494-596e9df2aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_step_num=0\n",
    "valid_prompt_types= ['text', 'image', 'command', 'dataentry']\n",
    "for curr_step in rpa_flow_steps:\n",
    "    curr_step_num= curr_step['Step']\n",
    "        \n",
    "    curr_step_type= curr_step['prompt_type']\n",
    "    if curr_step_num-prev_step_num <0:\n",
    "        print(curr_step)\n",
    "        print(\"Steps are not in increasing order. Exiting the program!\")\n",
    "        break\n",
    "    if curr_step_type not in valid_prompt_types:\n",
    "        print(curr_step)\n",
    "        print(\"Current Step's Prompt Type is Not Supported. Exiting the program! Choose 'text', 'image' , 'command' or 'dataentry' type and Rerun\")\n",
    "        break\n",
    "    prev_step_num= curr_step_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ecbc3-5609-454c-bce1-14917007fc92",
   "metadata": {},
   "source": [
    "### Text Prompt Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d9509-2e21-4b81-8219-2d5bfefe7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_text_location_screenshot(input_scr_shot_path, search_text, step_num, text_case_convert=True):\n",
    "\n",
    "    # Perform OCR on the Screenshot, Search for the Text and Return its Location\n",
    "    ocr_start_time = time.time()\n",
    "    \n",
    "    # Load the Screenshot\n",
    "    curr_screenshot_doc = DocumentFile.from_images(input_scr_shot_path)\n",
    "    ocr_result = docTR_ocr_model(curr_screenshot_doc)\n",
    "    \n",
    "    ocr_complete_time = time.time()\n",
    "    ocr_time_duration_sec = round(ocr_complete_time - ocr_start_time)\n",
    "    print(\"OCR on the Screenshot is Complete. Time taken in Seconds:\",ocr_time_duration_sec)\n",
    "    \n",
    "    # Convert the text to search into lower case\n",
    "    if text_case_convert:\n",
    "        search_text= search_text.lower()\n",
    "    \n",
    "    # Export the OCR Results and flatten it to find the keyword\n",
    "    json_output = ocr_result.export()\n",
    "    page_words = [[word for block in page['blocks'] for line in block['lines'] for word in line['words']] for page in json_output['pages']]\n",
    "    page_dims = [page['dimensions'] for page in json_output['pages']]\n",
    "    \n",
    "    # Get all the ocr text coordinates in absolute dimensions, format in [xmin, ymin, xmax, ymax]\n",
    "    words_abs_coords = [\n",
    "        [[int(round(word['geometry'][0][0] * dims[1])), int(round(word['geometry'][0][1] * dims[0])), int(round(word['geometry'][1][0] * dims[1])), int(round(word['geometry'][1][1] * dims[0]))] for word in words]\n",
    "        for words, dims in zip(page_words, page_dims)\n",
    "    ]\n",
    "    \n",
    "    # Index 0 indicates 1st page, as this function is per screenshot, the index is always [0]\n",
    "    all_words_ocr= page_words[0]\n",
    "    all_words_coords= words_abs_coords[0]\n",
    "    num_words= len(all_words_ocr)\n",
    "    \n",
    "    # Create directory for current_step outputs if not exists\n",
    "    curr_step_directory = \"step_\"+str(step_num) \n",
    "    flow_curr_step_scr_segments_path= os.path.join(flow_scr_segments_store_path, curr_step_directory)\n",
    "    print(\"Creating Directory for Current Step Outputs:\",flow_curr_step_scr_segments_path)\n",
    "    os.makedirs(flow_curr_step_scr_segments_path, exist_ok=True)\n",
    "    \n",
    "    keyword_match_locs= []\n",
    "    curr_screenshot_image = cv2.imread(input_scr_shot_path)\n",
    "    for word_ind in range(num_words):\n",
    "        curr_word_location= all_words_coords[word_ind]\n",
    "        x1, y1, x2, y2 = curr_word_location\n",
    "        cropped_image = curr_screenshot_image[y1:y2, x1:x2]\n",
    "        cropped_image_filename = os.path.join(flow_curr_step_scr_segments_path, str(word_ind) + '.png')\n",
    "        cv2.imwrite(cropped_image_filename, cropped_image)\n",
    "        \n",
    "        curr_word_value= all_words_ocr[word_ind]['value']\n",
    "        if text_case_convert:\n",
    "            curr_word_value= curr_word_value.lower()\n",
    "            \n",
    "        if search_text in curr_word_value:\n",
    "            keyword_match_locs.append(curr_word_location)\n",
    "            #print(word_ind)\n",
    "    \n",
    "    return keyword_match_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114019e-840f-4efe-aaca-55f2984063a7",
   "metadata": {},
   "source": [
    "### Image Prompt Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de2fef1-b2af-4d8a-a3fb-53954fb9c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_image_location_screenshot(input_scr_shot_path, search_image_path, step_num, image_sim_threshold=1):\n",
    "    curr_step_screenshot = cv2.imread(input_scr_shot_path)\n",
    "    curr_step_screenshot_sam = cv2.cvtColor(curr_step_screenshot, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    get_masks_start_time = time.time()\n",
    "    masks = sam_mask_generator.generate(curr_step_screenshot_sam)\n",
    "    get_masks_end_time = time.time()\n",
    "    \n",
    "    mask_generation_duration_sec = round(get_masks_end_time - get_masks_start_time)\n",
    "    print(\"Generating Image Patches/Masks for the Screenshot is Complete. Time taken in Seconds:\",mask_generation_duration_sec)\n",
    "\n",
    "    # Create directory for current_step outputs if not exists\n",
    "    curr_step_directory = \"step_\"+str(step_num) \n",
    "    flow_curr_step_scr_segments_path= os.path.join(flow_scr_segments_store_path, curr_step_directory)\n",
    "    print(\"Creating Directory for Current Step Outputs:\",flow_curr_step_scr_segments_path)\n",
    "    os.makedirs(flow_curr_step_scr_segments_path, exist_ok=True)\n",
    "    \n",
    "    # Extract all the Image Patches identified by SAM- Crop out all masks\n",
    "    for mask_ind in range(len(masks)):\n",
    "        x, y, width, height = masks[mask_ind]['bbox']\n",
    "        cropped_image = curr_step_screenshot[int(y):int(y+height), int(x):int(x+width)]\n",
    "        cropped_image_filename = os.path.join(flow_curr_step_scr_segments_path, str(mask_ind) + '.png')\n",
    "        cv2.imwrite(cropped_image_filename, cropped_image)\n",
    "    \n",
    "    # Perform Template Matching to Search for Images \n",
    "    # Template Matching with Pixels like OpenCV Template Matching is not Robust\n",
    "    ## Ref: https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html    \n",
    "\n",
    "    # Template Matching with Features like DinoV2 Features is Robust\n",
    "    ## Ref: https://github.com/facebookresearch/dinov2\n",
    "    template_image = Image.open(search_image_path)\n",
    "    \n",
    "    # Extract DinoV2 features for Template Image\n",
    "    with torch.no_grad():\n",
    "        inputs_dinov2_template_image = dinov2_processor(images=template_image, return_tensors=\"pt\").to(device)\n",
    "        outputs_dinov2_template_image = dinov2_model(**inputs_dinov2_template_image)\n",
    "        template_image_features_h = outputs_dinov2_template_image.last_hidden_state\n",
    "        template_image_features = template_image_features_h.mean(dim=1)\n",
    "    \n",
    "    # Calculate Similarity of the Query Image against Template Image using Cosine Similarity \n",
    "    cos_sim_score_list=[]\n",
    "    for mask_ind in range(len(masks)):\n",
    "        fname=f\"{flow_curr_step_scr_segments_path}/{mask_ind}.png\"\n",
    "        query_image = Image.open(fname)\n",
    "        \n",
    "        # Extract DinoV2 features for all Query Images: Patches/Crops\n",
    "        with torch.no_grad():\n",
    "            inputs_dinov2_query_image = dinov2_processor(images=query_image, return_tensors=\"pt\").to(device)\n",
    "            outputs_dinov2_query_image = dinov2_model(**inputs_dinov2_query_image)\n",
    "            query_image_patch_features_h = outputs_dinov2_query_image.last_hidden_state\n",
    "            query_image_patch_features = query_image_patch_features_h.mean(dim=1)\n",
    "            \n",
    "        # Compute the cosine similarity between image feature vectors, then scale it into 0-1 range\n",
    "        cos_fn = nn.CosineSimilarity(dim=0)\n",
    "        sim_score = cos_fn(template_image_features[0],query_image_patch_features[0]).item()\n",
    "        sim_score = (sim_score+1)/2\n",
    "        #print('Template and Query Image Patch's Similarity Score is:', sim_score)\n",
    "        cos_sim_score_list.append(sim_score)\n",
    "    \n",
    "    #Assumption- it is assumed that the template image being searched for will atleast match one location in the query image. \n",
    "    max_sim_val= max(cos_sim_score_list)\n",
    "    threshold_sim_val= image_sim_threshold * max_sim_val\n",
    "    image_match_locs = []\n",
    "    \n",
    "    for mask_ind in range(len(masks)):\n",
    "        curr_sim_val= cos_sim_score_list[mask_ind]\n",
    "        \n",
    "        if curr_sim_val >= threshold_sim_val:\n",
    "            x, y, width, height = masks[mask_ind]['bbox']\n",
    "            curr_mask_location = [x,y, x+width, y+height]\n",
    "            image_match_locs.append(curr_mask_location)\n",
    "            #print(mask_ind)\n",
    "    \n",
    "    return image_match_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba46ef35-a68c-4257-9f66-88384ba2bdea",
   "metadata": {},
   "source": [
    "### Run the RPA Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3f574-ef21-40fe-8013-c170748b9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To-do: \n",
    "\n",
    "#1. Support for Basic Auth\n",
    "##username= pyautogui.prompt(text='', title='' , default='')\n",
    "##password= pyautogui.password(text='', title='', default='', mask='*')\n",
    "\n",
    "#2. For Text Matching add support for regex pattern \n",
    "\n",
    "#3. Steps 4-8 happen on Same Screeshot- Add support for multi-step actions.\n",
    "## Single Screen-shot Multi Element Detection for Same Prompt Type     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e36d9-0bef-4ed0-93f7-4ed678ffcdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_step in rpa_flow_steps:\n",
    "    # Capture Screenshot for the Current Step in the RPA Flow\n",
    "    curr_step_num= curr_step['Step']\n",
    "    curr_scr_shot_filename = os.path.join(flow_screenshot_store_path, str(curr_step_num) + '.png')\n",
    "    pyautogui.screenshot(curr_scr_shot_filename)\n",
    "    \n",
    "    # Wait for 5 Seconds\n",
    "    time.sleep(5)\n",
    "    \n",
    "    curr_step_type= curr_step['prompt_type']\n",
    "    curr_step_prompt_content= curr_step['prompt_content']\n",
    "    curr_step_action= curr_step['action']\n",
    "    \n",
    "    if curr_step_type == 'text' or curr_step_type == 'image':\n",
    "        \n",
    "        if curr_step_type == 'text':\n",
    "            match_locs= retrieve_text_location_screenshot(curr_scr_shot_filename, curr_step_prompt_content, curr_step_num) \n",
    "        else:\n",
    "            match_locs= retrieve_image_location_screenshot(curr_scr_shot_filename, curr_step_prompt_content, curr_step_num)\n",
    "        \n",
    "        # Multiple matches -Tiebreak criteria  min (default), max, max_x, max_y, min_x, min_y (not handled)\n",
    "        print(match_locs)\n",
    "        if \"tie_break\" in curr_step.keys(): \n",
    "            match_location= match_locs[-1]\n",
    "        else:\n",
    "            match_location= match_locs[0]\n",
    "            \n",
    "        xmin, ymin, xmax, ymax= match_location\n",
    "        cx = (xmin + xmax) // 2\n",
    "        cy = (ymin + ymax) // 2\n",
    "\n",
    "        if curr_step_action == 'click':\n",
    "            # Move the mouse to cx, cy coordinates and click it.\n",
    "            pyautogui.click(cx, cy) \n",
    "            time.sleep(10)\n",
    "            print(f\"For Step {curr_step_num} performed mouse click at ({cx}, {cy})\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"For Step {curr_step_num} Invalid Action\")\n",
    "            continue #No Action is defined\n",
    "            \n",
    "    elif curr_step_type == 'dataentry':\n",
    "        pyautogui.write(curr_step_prompt_content)\n",
    "        print(f\"For Step {curr_step_num} performed data entry\")\n",
    "        \n",
    "    elif curr_step_type == 'command':\n",
    "        \n",
    "        if curr_step_action == 'enter':\n",
    "            pyautogui.write(curr_step_prompt_content)\n",
    "            print(f\"For Step {curr_step_num} performed data entry\")\n",
    "            time.sleep(10) #Add Delay/Wait\n",
    "            \n",
    "            pyautogui.press('enter') \n",
    "            time.sleep(10)\n",
    "            print(f\"For Step {curr_step_num} performed key press for enter\")\n",
    "            \n",
    "        elif curr_step_action == 'hotkey':\n",
    "            for keyDown in curr_step_prompt_content:\n",
    "                pyautogui.keyDown(keyDown)\n",
    "                print(f\"For Step {curr_step_num} performed hotkey {keyDown} key-down\")\n",
    "            for keyUp in reversed(curr_step_prompt_content):\n",
    "                pyautogui.keyUp(keyUp)\n",
    "                print(f\"For Step {curr_step_num} performed hotkey {keyUp} key-up\")\n",
    "\n",
    "        else:\n",
    "            print(f\"For Step {curr_step_num} Invalid Action\")\n",
    "            continue #No Action is defined\n",
    "            \n",
    "    else:\n",
    "        print(f\"For Step {curr_step_num} Invalid Action\")\n",
    "        continue #No Action is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91bfa3-0b0b-449c-b08b-c5941111232e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
